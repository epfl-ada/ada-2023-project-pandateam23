{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PandaTeam2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to investigate and elucidate key facets of our datasets, carry out preprocessing, and conduct preliminary analysis. The initial section of the notebook encompasses data loading and incorporates general preprocessing techniques utilized throughout the entire notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMARKS\n",
    "- Everybody: Error barsssss in plots\n",
    "- Everybody: Use Pearson coefficients to check for trends\n",
    "- Everybody : PEARSON COEFFICIENT\n",
    "\n",
    "\n",
    "- Diego: parle de **cumulative inflation == ?** dans les methodes peut etre\n",
    "- André + Nicolas: How do we handle missing dates?(can we scrape them?)\n",
    "\n",
    "- Diego: Explique que: \"for now the constants we're using are just empirical constants, we will need to tune them after\" = > **Threshold values for example ?**\n",
    "\n",
    "- Diego: Parle de Social media dataset (si onn trouve une qui existe) et voir si ca valide nos predictions\n",
    "\n",
    "\n",
    "CHANGES NOT TO FORGET\n",
    "- for fame Mean or sum?\n",
    "\n",
    "actor_movie_year = actor_popularity_movie_coef.groupby(['Actor Name', 'Year'])['Recognition Coefficient'].mean().reset_index()\n",
    "\n",
    "- Change 20 years to 5 years\n",
    "\n",
    "\n",
    "- These were not needed\n",
    "\n",
    "actor_movie_year['Year'] = actor_movie_year['Year'].replace(1010, 2010)\n",
    "actor_movie_year = actor_movie_year[actor_movie_year['Year'] >= 1914]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "\n",
    "- OK André + Pico: Think about How to Normalize?\n",
    "\n",
    "- OK André + Pico:Pico: See if there are other visualizations we could use\n",
    "\n",
    "- OK André + Pico: Any pearson coefficient to show trends\n",
    "\n",
    "- OK André + Pico: Plot revenue before normalization to show it's heavy tailed\n",
    "\n",
    "- OK Pico: Look at last block (See if it's interesting)\n",
    "\n",
    "actor_year_df.groupby('Year').mean().reset_index().plot(x='Year',y='Recognition Coefficient')\n",
    "- OK André: lower case actors?\n",
    "\n",
    "- OK André: How many famous actors per year?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Contents of notebook:**\n",
    "- [Data Processing](#Data-Processing)\n",
    "  - [Loading data](#Loading-data)\n",
    "  - [Merging the dataframes](#Merging-the-dataframes)\n",
    "    - [Prepare the inflation dataframe](#Prepare-the-inflation-dataframe)\n",
    "    - [Merge given dataframes](#Merge-given-dataframes)\n",
    "    - [Enrich dataframe with other datasets](#Enrich-dataframe-with-other-datasets)\n",
    "    - [Adapt Box office with inflation](#Adapt-Box-office-with-inflation)\n",
    "- [Actor recognition](#Actor-recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime as dt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/MovieSummaries/character.metadata.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m character \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/MovieSummaries/character.metadata.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m movie \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/MovieSummaries/movie.metadata.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m plot_summaries \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/MovieSummaries/plot_summaries.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/MovieSummaries/character.metadata.tsv'"
     ]
    }
   ],
   "source": [
    "character = pd.read_csv('./data/MovieSummaries/character.metadata.tsv', sep='\\t', header=None)\n",
    "movie = pd.read_csv('./data/MovieSummaries/movie.metadata.tsv', sep='\\t', header=None)\n",
    "plot_summaries = pd.read_csv('./data/MovieSummaries/plot_summaries.txt', sep='\\t', header=None)\n",
    "IMDb_data = pd.read_csv('./data/IMDb/akas.tsv', sep='\\t', dtype={7: str})\n",
    "IMDb_ratings = pd.read_csv('./data/IMDb/ratings.tsv', sep='\\t')\n",
    "TheMoviesDataset = pd.read_csv('./data/TheMoviesDataset/movies_metadata.csv')\n",
    "mojo_int1000 = pd.read_csv('./data/Mojo/boxofficemojointernationaltop1000.tsv', sep='\\t')\n",
    "mojo_us1000 = pd.read_csv('./data/Mojo/boxofficemojoustop1000.tsv', sep='\\t')\n",
    "inflation = pd.read_excel('./data/Inflation/Inflation-data.xlsx', sheet_name='hcpi_a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding column names\n",
    "movie_columns = [\"Wikipedia movie ID\", \"Freebase movie ID\", \"Movie name\", \"Movie release date\", \"Movie box office revenue\", \"Movie runtime\", \"Movie languages (Freebase ID:name tuples)\", \"Movie countries (Freebase ID:name tuples)\", \"Movie genres (Freebase ID:name tuples)\"]\n",
    "character_columns = [\"Wikipedia movie ID\", \"Freebase movie ID\", \"Movie release date\", \"Character name\", \"Actor date of birth\", \"Actor gender\", \"Actor height (in meters)\", \"Actor ethnicity (Freebase ID)\", \"Actor name\", \"Actor age at movie release\", \"Freebase character/actor map ID\", \"Freebase character ID\", \"Freebase actor ID\"]\n",
    "plot_summaries_columns = [\"Wikipedia movie ID\", \"Plot summary\"]\n",
    "\n",
    "movie.columns = movie_columns\n",
    "character.columns = character_columns\n",
    "plot_summaries.columns = plot_summaries_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the dataframes\n",
    "Several datasets are used in this notebook, we need to merge several dataframes. Namely ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the inflation dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Inflation-data.xlsx` is an additional dataset, contains inflation rates over time and location. However, it has to be adapted in order to be used in our analysis e.g. adapt the time-span, fix the location of the inflation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering inflation in the US as a baseline\n",
    "inflation_us = inflation[inflation['Country Code']=='USA']\n",
    "\n",
    "# Melt the DataFrame to reshape it\n",
    "melted_inflation = pd.melt(inflation_us, id_vars=['Country Code', 'IMF Country Code', 'Country', 'Indicator Type', 'Series Name'], var_name='Year', value_name='Inflation')\n",
    "\n",
    "# Convert the \"Year\" column to numeric\n",
    "melted_inflation['Year'] = pd.to_numeric(melted_inflation['Year'], errors='coerce')\n",
    "\n",
    "# Select relevant columns\n",
    "melted_inflation = melted_inflation[['Year','Inflation']]\n",
    "melted_inflation = melted_inflation.iloc[:-2]\n",
    "\n",
    "# Create a DataFrame with years from 1914 to 1969 and 'inflation' set to 0 (To account for missing inflation values)\n",
    "additional_years = pd.DataFrame({'Year': range(1914, 1970), 'Inflation': 0})\n",
    "\n",
    "# Add a year 0 for missing years\n",
    "additional_years = additional_years.append({'Year':0,'Inflation':0}, ignore_index=True)\n",
    "\n",
    "# Merge the additional_years DataFrame with melted_inflation (Adding 0 inflation to missing years)\n",
    "melted_inflation = pd.concat([melted_inflation, additional_years], ignore_index=True, sort=False)\n",
    "\n",
    "# Sort the DataFrame\n",
    "melted_inflation = melted_inflation.sort_values(by='Year')\n",
    "\n",
    "# Calculate cumulative inflation to adapt revenues further\n",
    "melted_inflation['Cumulative Inflation'] = (melted_inflation['Inflation'].astype(float)/100 + 1).cumprod()\n",
    "\n",
    "melted_inflation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge given dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge movies and actors based on the Movie ID\n",
    "movie_actor = pd.merge(movie, character, how=\"left\", on=[\"Wikipedia movie ID\",\"Freebase movie ID\"])\n",
    "\n",
    "# Merge IMDb ratings with the corresponding Movie title\n",
    "IMDb_combined = pd.merge(IMDb_data,IMDb_ratings,left_on='titleId',right_on='tconst')\n",
    "\n",
    "# Getting lower cases movie title to merge on titles\n",
    "movie_actor['title_lower'] = movie_actor['Movie name'].str.lower()\n",
    "IMDb_combined['title_lower'] = IMDb_combined['title'].str.lower()\n",
    "\n",
    "# Average different ratings for same Movie\n",
    "IMDb_combined_mean = IMDb_combined[['title_lower','averageRating']].groupby(['title_lower']).mean()\n",
    "IMDb_combined_mean.reset_index(inplace=True)\n",
    "\n",
    "# Merge IMDb ratings with the movies and actors dataframe\n",
    "movie_actor_IMDb = pd.merge(movie_actor,IMDb_combined_mean[['title_lower','averageRating']],on='title_lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enrich dataframe with other datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add missing Box office values and missing release dates from other datasets ([Mojo](https://www.kaggle.com/datasets/kalilurrahman/top-box-office-revenue-data-english-movies/data?select=boxofficemojoustop1000.tsv) & [The Movies Datset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?resource=download&select=movies_metadata.csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of datasets to merge\n",
    "datasets = [\n",
    "    (TheMoviesDataset, 'original_title', 'revenue','release_date'),\n",
    "    (mojo_us1000, 'Movie', 'Lifetime Gross','Year'),\n",
    "    (mojo_int1000, 'Title', 'Worldwide Lifetime Gross','Year')\n",
    "]\n",
    "\n",
    "# Show that number of Nan values decreases\n",
    "print(\"Numer of missing Box office values\",movie_actor_IMDb['Movie box office revenue'].isna().sum())\n",
    "print(\"Numer of missing release dates values\",movie_actor_IMDb['Movie box office revenue'].isna().sum())\n",
    "print(\"\")\n",
    "\n",
    "# Iterate through the datasets and merge\n",
    "for dataset, movie_column, revenue_column, date_column in datasets:\n",
    "    # Remove rows that have revenue = 0\n",
    "    dataset=dataset[dataset[revenue_column]!=0.0]\n",
    "\n",
    "    # Getting lower cases movie title to merge on titles\n",
    "    dataset = dataset.copy()\n",
    "    dataset['title_lower'] = dataset[movie_column].str.lower()\n",
    "\n",
    "    # Grouping same movie names of dataset\n",
    "    dataset = dataset.groupby('title_lower').agg({f'{revenue_column}': 'mean', f'{date_column}': 'first'}).reset_index()\n",
    "    \n",
    "    #####################\n",
    "    ##################### not sure how well written this is, besides merging on a title without year is not effective\n",
    "    #####################\n",
    "    \n",
    "    # Add missing values\n",
    "    movie_actor_IMDb = pd.merge(movie_actor_IMDb, dataset[['title_lower', revenue_column, date_column]], on='title_lower', how='left')\n",
    "    movie_actor_IMDb['Movie box office revenue'] = movie_actor_IMDb['Movie box office revenue'].fillna(movie_actor_IMDb[revenue_column])\n",
    "    movie_actor_IMDb['Movie release date_x'] = movie_actor_IMDb['Movie release date_x'].fillna(movie_actor_IMDb[date_column])\n",
    "    movie_actor_IMDb.drop([revenue_column,date_column], axis=1,inplace=True)\n",
    "\n",
    "    # Show that number of Nan values decreases\n",
    "    print(\"Numer of missing Box office values\",movie_actor_IMDb['Movie box office revenue'].isna().sum())\n",
    "    print(\"Numer of missing release dates values\",movie_actor_IMDb['Movie release date_x'].isna().sum())\n",
    "    print(\"\")\n",
    "\n",
    "movie_actor_IMDb.drop_duplicates(subset=['Actor name', 'Movie name', 'Movie release date_x'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address missing data in `Movie release date` and `Movie box office revenue` without significant information loss, NaN values are replaced with zero, allowing retention of valuable data points in the dataset.\n",
    "\n",
    "**WHAT ABOUT MISISNG VALUES OF ETHNITICYTY AND GENDER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Nans to 0\n",
    "movie_actor_IMDb['Movie box office revenue'] = movie_actor_IMDb['Movie box office revenue'].fillna(0)\n",
    "movie_actor_IMDb['Movie release date_x'] = movie_actor_IMDb['Movie release date_x'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, there are some `Actor name` missing. Since our analysis relies on actors, if the actor's name is missing, then the corresponding row wouldn't be useful. Therefore we remove rows that have missing actor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_actor_IMDb['Actor name'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that have missing actor names\n",
    "movie_actor_IMDb.dropna(subset='Actor name', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt Box office with inflation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for temporal variations in the value of the US dollar, an adjustment is applied to box office revenues by normalizing them against cumulative inflation. This involves dividing the movie's box office revenue by the cumulative inflation factor, ensuring a comparable scale over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Movie release date from string to date\n",
    "movie_actor_IMDb[\"Movie release date_x\"] = pd.to_datetime(\n",
    "    movie_actor_IMDb[\"Movie release date_x\"], errors=\"coerce\", utc=True\n",
    ")\n",
    "# Extract year from Movie release date\n",
    "movie_actor_IMDb[\"Year\"] = pd.DatetimeIndex(movie_actor_IMDb[\"Movie release date_x\"]).year\n",
    "\n",
    "movie_actor_IMDb_inflation = pd.merge(movie_actor_IMDb,melted_inflation[['Year','Cumulative Inflation']])\n",
    "\n",
    "# Calculate Adapted Movie Box Office Revenue  \n",
    "movie_actor_IMDb_inflation['Adapted Movie box office revenue'] = movie_actor_IMDb_inflation['Movie box office revenue']/movie_actor_IMDb_inflation['Cumulative Inflation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current dataframe structure consists of unique rows corresponding to movie-actor pairs.\n",
    "\n",
    "For each row, the important data for further analysis is:\n",
    "- The Inflation Adapted Movie box office revenue\n",
    "- The rating of the movie\n",
    "- The release date of the movie\n",
    "- The age of the actor and his date of birth\n",
    "- The ethnicity of the actor\n",
    "- The gender of the actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Actor recognition\n",
    "DIEGO: Parler un peu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserve relevant columns\n",
    "columns_to_conserve = ['Actor name', 'Movie name', 'Movie release date_x', 'averageRating',\\\n",
    "                       'Adapted Movie box office revenue','Actor date of birth','Actor gender',\\\n",
    "                        'Actor ethnicity (Freebase ID)','Actor age at movie release' ,'Year']\n",
    "\n",
    "conserved_df = movie_actor_IMDb_inflation[columns_to_conserve].copy()\n",
    "\n",
    "conserved_df.columns = ['Actor Name', 'Movie name', 'Date', 'Rating',\\\n",
    "                       'Revenue','Actor date of birth','Actor gender',\\\n",
    "                        'Actor ethnicity (Freebase ID)','Actor age at movie release' ,'Year']\n",
    "\n",
    "conserved_df = conserved_df.sort_values(by='Year', ascending=False)\n",
    "\n",
    "conserved_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove actors who have less than 5 movies\n",
    "As an initial preprocessing measure, we filter out actors with fewer than 5 movie appearances. This step is undertaken to optimize computational efficiency, as it is assumed that actors with a limited number of film credits may not possess significant recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove actors who have less than 5 movies\n",
    "actor_movie_counts = conserved_df['Actor Name'].value_counts()\n",
    "actors_with_5_or_more_movies = actor_movie_counts[actor_movie_counts >= 5].index\n",
    "filtered_df = conserved_df[conserved_df['Actor Name'].isin(actors_with_5_or_more_movies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize rating and revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing that revenue is heavy-tailed right skewed\n",
    "\n",
    "# Remove zeros Revenues (missing revenues)\n",
    "filtered_df_no_0s = filtered_df[filtered_df['Revenue'] > 1]\n",
    "\n",
    "revenue_array = plt.hist(filtered_df_no_0s['Revenue'], bins=1000000, density=True, cumulative=-1,\n",
    "                               color='blue', histtype='step', label=\"Revenue\")\n",
    "plt.xlabel('Movie Box Office [$]')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('CCDF of Revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(revenue_array[1][1:], revenue_array[0], label=\"Left-handed pitchers\", color='blue')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Movie Box Office [$]')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('CCDF of Revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing that rating is normal-like\n",
    "filtered_df.hist(['Rating'],bins=50)\n",
    "plt.xlabel('IMDb Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rating Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the previous plots:\n",
    "- The Revenue of movies follows a right-skewed heavy-tailed distribution\n",
    "- The Rating of movies follows a normal-like distribution\n",
    "\n",
    "In order to normalize/scale these distributions, we use relevant techniques for each distribution:\n",
    "- For a heavy-tailed distribution, we apply a logarithmic transformation\n",
    "- For a normal distribution, we apply Z-score normalization\n",
    "\n",
    "After these two normalizations, we then apply Min/Max scaling to have the same scale for the two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming filtered_df is your DataFrame\n",
    "columns_to_scale = ['Revenue', 'Rating']\n",
    "actor_popularity_rank = filtered_df.copy()\n",
    "\n",
    "# Z-score normalization for Rating then min max scaling\n",
    "actor_popularity_rank['Rating'] = (actor_popularity_rank['Rating'] - actor_popularity_rank['Rating'].mean()) / actor_popularity_rank['Rating'].std()\n",
    "actor_popularity_rank['Rating'] = (actor_popularity_rank['Rating']-actor_popularity_rank['Rating'].min())/(actor_popularity_rank['Rating'].max()-actor_popularity_rank['Rating'].min())\n",
    "\n",
    "\n",
    "# log normalization of Revenue then min max scaling\n",
    "actor_popularity_rank.loc[actor_popularity_rank['Revenue'] < 1, 'Revenue'] = 1\n",
    "actor_popularity_rank['Revenue'] = actor_popularity_rank['Revenue'].apply(lambda x: np.log(x))\n",
    "actor_popularity_rank['Revenue'] = (actor_popularity_rank['Revenue']-actor_popularity_rank['Revenue'].min())/(actor_popularity_rank['Revenue'].max()-actor_popularity_rank['Revenue'].min())\n",
    "\n",
    "# Sort by 'Year' in descending order\n",
    "actor_popularity_rank = actor_popularity_rank.sort_values(by='Year', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "actor_popularity_rank.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Actor Recognition Coefficient for each (Movie, Actor) pair\n",
    "We define the Actor Recognition Coefficient to be a weighted average between the movie revenue and its rating.\n",
    "\n",
    "DIEGO: WRITE FORMULA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Actor Recognition Coefficient for each (Movie, Actor) pair\n",
    "columns_to_scale = ['Revenue', 'Rating']\n",
    "\n",
    "#  Weights can be adjusted\n",
    "weights = [1, 1]\n",
    "actor_popularity_movie_coef = actor_popularity_rank\n",
    "\n",
    "# Calculate weighted average of normalized rating and normalized revenue\n",
    "actor_popularity_movie_coef['Recognition Coefficient'] = np.average(actor_popularity_rank[columns_to_scale], axis=1, weights=weights)\n",
    "\n",
    "actor_popularity_movie_coef = actor_popularity_movie_coef.sort_values(by='Recognition Coefficient',ascending=False)\n",
    "\n",
    "actor_popularity_movie_coef.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Actor Recognition Coefficient for every Actor every Year\n",
    "This is done by taking the mean of the Recognition Coefficient of the movies the Actor played in each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Actor Recognition Coefficient for every Actor every Year\n",
    "actor_movie_year = actor_popularity_movie_coef.groupby(['Actor Name', 'Year'])['Recognition Coefficient'].mean().reset_index()\n",
    "\n",
    "actor_movie_year = actor_movie_year.sort_values(by='Year',ascending=True)\n",
    "\n",
    "actor_movie_year.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the dataset to include entries for all actors in each year\n",
    "We're creating a DataFrame with all actor-year combinations from 1914 to 2012, putting the Actor Recognition Coefficient to zero if the actor does not have films in the corresponding year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all actor-year combinations from 1914 to 2012\n",
    "years = list(range(1914, 2013))\n",
    "\n",
    "# Get unique actor names \n",
    "actors = actor_movie_year['Actor Name'].unique()\n",
    "\n",
    "# Create a MultiIndex DataFrame with all possible actor-year combinations\n",
    "actor_year_combinations = pd.MultiIndex.from_product([actors, years], names=['Actor Name', 'Year']).to_frame(index=False)\n",
    "actor_year_combinations.columns = ['Actor Name', 'Year']\n",
    "\n",
    "# Merge the actor_year_combinations DataFrame with actor_movie_year to get the coefficients\n",
    "all_actor_year = pd.merge(actor_year_combinations, actor_movie_year, on=['Actor Name', 'Year'], how='left')\n",
    "\n",
    "# Fill missing values in the 'Recognition Coefficient' column with 0\n",
    "all_actor_year['Recognition Coefficient'].fillna(0, inplace=True)\n",
    "\n",
    "# Rename 'Recognition Coefficient' to 'Actor Year Coefficient'\n",
    "all_actor_year.rename(columns={'Recognition Coefficient': 'Recognition Coefficient'}, inplace=True)\n",
    "\n",
    "# Sort the result DataFrame\n",
    "all_actor_year = all_actor_year.sort_values(by=['Year', 'Actor Name'], ascending=[True, True])\n",
    "\n",
    "# Reset the index\n",
    "all_actor_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(all_actor_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe, that has a Recognition Coefficient value for each possible (Actor Name, Year) combination.\n",
    "### Recognition of Past Years\n",
    "Even if a certain actors does not play in any movie in a certain year, We consider that this actor would remain well-recognized to a certain extent during this year. To account for this effect, we add a certain fraction of the past year's Recognition Coefficient to the the current year's Recognition Coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognition of Past Years\n",
    "previous_year_contribution = 0.5\n",
    "\n",
    "actor_year_df = all_actor_year.copy()\n",
    "actor_year_df.sort_values(by=['Actor Name', 'Year'], inplace=True)\n",
    "\n",
    "# Initialize a dictionary to store the previous 'Coefficient with Past' values for each actor\n",
    "prev_b_values = {}\n",
    "\n",
    "# Iterate through the rows of the DataFrame to compute the values for column 'Coefficient with Past'\n",
    "for index, row in actor_year_df.iterrows():\n",
    "    actor = row['Actor Name']\n",
    "    year = row['Year']\n",
    "    a_value = row['Recognition Coefficient']\n",
    "    \n",
    "    if actor in prev_b_values:\n",
    "        b_value = a_value + previous_year_contribution * prev_b_values[actor]\n",
    "    else:\n",
    "        b_value = a_value  # If there is no previous value, use A(actor, year) as-is\n",
    "    \n",
    "    actor_year_df.at[index, 'Coefficient with Past'] = b_value\n",
    "    prev_b_values[actor] = b_value\n",
    "\n",
    "actor_year_df['Year'] = pd.to_numeric(actor_year_df['Year'])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(actor_year_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General recognition\n",
    "Now that we have the Actor Recognition Coefficient for every Actor in every Year, we can calculate the General Actor Recognition Coefficient by calculating the mean of the Recognition Coefficient through the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_recognition = actor_year_df.groupby(['Actor Name'])['Recognition Coefficient'].mean().reset_index()\n",
    "general_recognition = general_recognition.sort_values(by='Recognition Coefficient',ascending=False)\n",
    "(general_recognition).head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIEGO: We can see that the most well-recognized actors are indeed actors that are well-known ....\n",
    "Some of them are modern-day actors like Robert De Niro. However, we can also see that older well-recognized actors are accounted for as we find actors like John Wayne and Sean Connery\n",
    "\n",
    "CONTINUE ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Recognition Span Evolution Through time\n",
    "In this section, the idea is to examine the span of the actors' Recognition through time. In other words, we are trying to find the year in which he started to be well-recognized and the year this recognition \"ended\". We do that by selecting the year range in which the Actor Recognition Coefficient is above a certain threshold.\n",
    "\n",
    "DIEGO: CHECK AND COMPLETE add that as next steps we will basically have to identify a threshold values for fame by analyzing the dirstirbution. Importnat however to remmember at the begnning we filtered nly actors that played in more than 10 movies\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative threshold for every actor, threshold being 30% of the Actor's Maximum Recognition\n",
    "\n",
    "# Calculate fame threshold for each actor, threshold 50% of the maximum Recognition of this actor\n",
    "fame_start_end_threshold = actor_year_df.groupby('Actor Name').max().reset_index()[['Actor Name','Recognition Coefficient']]\n",
    "fame_start_end_threshold['threshold'] = 0.5*fame_start_end_threshold['Recognition Coefficient']\n",
    "fame_start_end_threshold.drop(columns='Recognition Coefficient', inplace=True)\n",
    "\n",
    "# Keep Recognition Values that are above the threshold\n",
    "merged_df = pd.merge(actor_year_df, fame_start_end_threshold, on='Actor Name', how='left')\n",
    "result_df = merged_df[merged_df['Recognition Coefficient'] > merged_df['threshold']]\n",
    "\n",
    "# Find the minimum (first) and maximum (last) Year corresponding to these values\n",
    "grouped = result_df.groupby('Actor Name').agg({'Year': ['min', 'max']})\n",
    "grouped = grouped.reset_index()\n",
    "grouped.columns = ['Actor Name', 'First Year', 'Last Year']  # Rename the columns\n",
    "result_df = grouped.copy()  # Create a copy of grouped DataFrame\n",
    "\n",
    "# Calculate the Range (last year - first year)\n",
    "result_df['Range'] = result_df['Last Year'] - result_df['First Year']\n",
    "result_df = result_df.sort_values(by='First Year')\n",
    "\n",
    "result_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group ranges for each first year by taking the mean and standard deviation of the range\n",
    "filtered_actor_year_df = result_df.sort_values(by='First Year')\n",
    "grouped_years = filtered_actor_year_df.groupby('First Year')['Range'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "grouped_years.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results about Actor Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Average Span of Recognition through the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(grouped_years['First Year'],grouped_years['mean'],label='Mean')\n",
    "plt.fill_between(grouped_years['First Year'], grouped_years['mean'] - grouped_years['std'], grouped_years['mean'] + grouped_years['std'], alpha=0.3, label='Std Dev')\n",
    "plt.title('Average Span of Recognition through the Years')\n",
    "plt.xlabel('First Year')\n",
    "plt.ylabel('Average Span of Recognition [Years]')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data before and after 1970\n",
    "before_1970 = grouped_years[grouped_years['First Year'] < 1970]\n",
    "after_1970 = grouped_years[grouped_years['First Year'] >= 1970]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting regression line for before 1970\n",
    "regplot_before_1970 = sns.regplot(x='First Year', y=\"mean\", data=before_1970, ci=95, scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"}, label='Before 1970')\n",
    "# Plotting scatter points for after 1970\n",
    "plt.scatter(after_1970['First Year'], after_1970['mean'], color=\"blue\", marker=\"o\", label='After 1970')\n",
    "\n",
    "plt.title('Average Span of Recognition through the Years')\n",
    "plt.xlabel('First Year')\n",
    "plt.ylabel('Average Span of Recognition [Years]')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIEGO:!!\n",
    "\n",
    "ANALYZE THIS GRAPH (SEE IF THERE ARE OTHER RELEVANT GRAPHS)\n",
    "\n",
    "NOTE: starting about 1970 this range decrease since the data we have ends on 2012. Therefore actors starting to be recognized in 2012 (Having a First Year of 2012), would have a range of 0 years since their Last Year would also be considered to be 2012\n",
    "\n",
    "DIEGO: CHECK AND COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Value of Recognition coefficient through the Years\n",
    "grouped_data = actor_year_df.groupby('Year')['Recognition Coefficient'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(grouped_data['Year'], grouped_data['mean'], label='Mean')\n",
    "plt.fill_between(grouped_data['Year'], grouped_data['mean'] - grouped_data['std'], grouped_data['mean'] + grouped_data['std'], alpha=0.3, label='Std Dev')\n",
    "plt.title('Recognition through the years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Recognition Coefficient')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.regplot(\n",
    "    x='Year',\n",
    "    y=\"mean\",\n",
    "    data=grouped_data,\n",
    "    ci=95,\n",
    "    scatter_kws={\"color\": \"blue\"},\n",
    "    line_kws={\"color\": \"red\"},\n",
    ")\n",
    "plt.title('Recognition through the years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Recognition Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson Coefficient for before 1970\n",
    "print(stats.pearsonr(grouped_data['Year'],grouped_data['mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIEGO: CHECK & CONTINUE (SAY THAT WE SEE BIG STANDARD DEVIATION IN 1ST PLOT)\n",
    "\n",
    "From the previous Pearson test, since we have an extremely small p-value, we have strong evidence to reject the null-Hypothesis that suggests that there is no correlation between the Year and the Average Recognition Coefficient.\n",
    "This result should be further explored in P3, to find the reasons for this relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Recognition of a specific actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_names = ['Michael Caine','Robert De Niro','Clint Eastwood']\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, len(actor_names), figsize=(14, 4))\n",
    "\n",
    "\n",
    "for i, actor_name in enumerate(actor_names):\n",
    "    particular_actor_df = actor_year_df[actor_year_df['Actor Name'] == actor_name]\n",
    "    years = particular_actor_df['Year']\n",
    "    coefficients = particular_actor_df['Coefficient with Past']\n",
    "    axes[i].plot(years, coefficients)\n",
    "    axes[i].set_title(f'Recognition of {actor_name}')\n",
    "    axes[i].set_xlabel('Year')\n",
    "    axes[i].set_ylabel('Recognition Coefficient')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "    axes[i].set_xlim(1914, 2012)\n",
    "    axes[i].set_ylim(0, 1.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of well-recognized actors per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 180 actors (int(len(general_recognition)/100)) in general_recognition and check in which years they are well-recognized\n",
    "\n",
    "recognized_actors = general_recognition[:int(len(general_recognition)/100)]\n",
    "\n",
    "\n",
    "# Calculate fame threshold for each actor, threshold 50% of the maximum Recognition of this actor\n",
    "fame_start_end_threshold = actor_year_df.groupby('Actor Name').max().reset_index()[['Actor Name','Recognition Coefficient']]\n",
    "fame_start_end_threshold['threshold'] = 0.5*fame_start_end_threshold['Recognition Coefficient']\n",
    "fame_start_end_threshold.drop(columns='Recognition Coefficient', inplace=True)\n",
    "\n",
    "\n",
    "# Keep Years that are above the threshold\n",
    "actor_year_df2 = actor_year_df[actor_year_df['Actor Name'].isin(recognized_actors['Actor Name'])]\n",
    "merged_df = pd.merge(actor_year_df2, fame_start_end_threshold, on='Actor Name', how='left')\n",
    "result_df = merged_df[merged_df['Recognition Coefficient'] > merged_df['threshold']]\n",
    "\n",
    "result_df2 = result_df.groupby('Year').count().reset_index()[['Year','Actor Name']]\n",
    "\n",
    "\n",
    "plt.bar(result_df2['Year'],result_df2['Actor Name'])\n",
    "plt.title('Number of well-recognized actors per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIEGO: CHECK & CONTINUE\n",
    "\n",
    "We see that the majority of the well-recognized actors are in years that follow 1980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNUSED BUT MIGHT BE USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVED 6TH STEP BUT MIGHT BE USEFUL LATER\n",
    "## 6TH STEP: REMOVE ACTORS WHO HAVE HAD SUCCESS BEFORE 1924 OR AFTER 2002\n",
    "## THIS ALLOWS US TO REMOVE ACTORS WHO MIGHT STARTED ACTING BEFORE THE DATASET OR WHO CONTINUED ACTING AFTER\n",
    "mask = actor_year_df[(actor_year_df['Year'] >= 2002) & (actor_year_df['Coefficient with Past'] > 0.1)]\n",
    "actors_to_remove = mask['Actor Name'].unique()\n",
    "filtered_actor_year_df = actor_year_df[~actor_year_df['Actor Name'].isin(actors_to_remove)]\n",
    "\n",
    "mask = actor_year_df[(actor_year_df['Year'] <= 1924) & (actor_year_df['Coefficient with Past'] > 0.1)]\n",
    "actors_to_remove = mask['Actor Name'].unique()\n",
    "filtered_actor_year_df = filtered_actor_year_df[~actor_year_df['Actor Name'].isin(actors_to_remove)]\n",
    "filtered_actor_year_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(filtered_actor_year_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECKING FOR CORRECTNESS WITH Željko Ivanek IN 2012\n",
    "zelko_ivanek_df = actor_popularity_movie_coef[(actor_popularity_movie_coef['Actor Name'] == 'Željko Ivanek') & (actor_popularity_movie_coef['Year'] == 2012)]\n",
    "display(zelko_ivanek_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(revenue_array[1][1:], revenue_array[0], label=\"Left-handed pitchers\", color='blue')\n",
    "plt.xscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## 1ST STEP: RANKIZE RATING AND REVENUE\n",
    "# columns_to_scale = ['Revenue', 'Rating']\n",
    "# actor_popularity_rank = filtered_df.copy()\n",
    "# ranks = filtered_df[columns_to_scale].rank()\n",
    "# # normalized_ranks to be between 0 and 1\n",
    "# normalized_ranks = (ranks - ranks.min()) / (ranks.max() - ranks.min())\n",
    "# actor_popularity_rank[columns_to_scale] = normalized_ranks\n",
    "# actor_popularity_rank = actor_popularity_rank.sort_values(by='Year', ascending=False)\n",
    "\n",
    "# # display(actor_popularity_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Empirical threshold\n",
    "\n",
    "# fame_start_end_threshold = 0.1\n",
    "\n",
    "# Keep Recognition Values that are above the threshold\n",
    "# filtered_actor_year_df = actor_year_df[actor_year_df['Coefficient with Past'] > fame_start_end_threshold]\n",
    "# grouped = filtered_actor_year_df.groupby('Actor Name').agg({'Year': ['min', 'max']})\n",
    "# grouped = grouped.reset_index()\n",
    "# grouped.columns = ['Actor Name', 'First Year', 'Last Year']  # Rename the columns\n",
    "# result_df = grouped.copy()  # Create a copy of grouped DataFrame\n",
    "\n",
    "# Find the minimum (first) and maximum (last) Year corresponding to these values\n",
    "# Calculate the Range (last year - first year)\n",
    "# result_df['Range'] = result_df['Last Year'] - result_df['First Year']\n",
    "# result_df = result_df.sort_values(by='First Year')\n",
    "\n",
    "# display(result_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
